{
  "hash": "fe6b6e8164deead8f20bc0ef799d8a16",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ROL-PI Day two\"\ndescription: \"MÃ¡s CpGs, por favor\"\ncategories: [methylation, e5]\n#citation: \ndate: 05-24-2025\nimage: http://gannet.fish.washington.edu/seashell/snaps/Monosnap_EpigeneticsSimulationMS_-_Online_LaTeX_Editor_Overleaf_2025-05-25_07-13-14.png # finding a good image\n\nauthor:\n  - name: Steven Roberts\n    url: \n    orcid: 0000-0001-8302-1138\n    affiliation: Professor, UW - School of Aquatic and Fishery Sciences\n    affiliation-url: https://robertslab.info\n  #url:  # self-defined\ndraft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!\nformat:\n  html:\n    code-fold: FALSE\n    code-tools: true\n    code-copy: true\n    highlight-style: github\n    code-overflow: wrap\n#runtime: shiny\n---\n\n\n\n\n\nDay two. Much of time was spent on Epigenetic Simulation then moving on to POISE model.\n\nDid some slide edits...\n\nAnd coding to figure out what caused the lack of CpG loci in Apul (TLDR::one bad sample)\n\n# Trouble shooting Apul \n\nWent back to processed txt loci files (from 10x bedgraph).\n\nFound that 225-T1 was much smaller in line counts and we tracked down multiQC on bismark and found very bad alignment. \n\nTo remedy, did merge as before. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#pull processed files from Gannet \n# Note: Unfortunately we can't use the `cache` feature to make this process more time efficient, as it doesn't support long vectors\n\n# Define the base URL\nbase_url <- \"https://gannet.fish.washington.edu/seashell/bu-github/timeseries_molecular/D-Apul/output/15.5-Apul-bismark/\"\n\n# Read the HTML page\npage <- read_html(base_url)\n\n# Extract links to files\nfile_links <- page %>%\n  html_nodes(\"a\") %>%\n  html_attr(\"href\")\n\n# Filter for files ending in \"processed.txt\"\nprocessed_files <- file_links[grepl(\"processed\\\\.txt$\", file_links)]\n\n# Create full URLs\nfile_urls <- paste0(base_url, processed_files)\n\n# Function to read a file from URL\nread_processed_file <- function(url) {\n  read_table(url, col_types = cols(.default = \"c\"))  # Read as character to avoid parsing issues\n}\n\n# Import all processed files into a list\nprocessed_data <- lapply(file_urls, read_processed_file)\n\n# Name the list elements by file name\nnames(processed_data) <- processed_files\n\n# Print structure of imported data\nstr(processed_data)\n\n# add a header row that has \"CpG\" for the first column and \"sample\" for the second column, which will be populated by the file name \n\nprocessed_data <- Map(function(df, filename) {\n  colnames(df) <- c(\"CpG\", filename)  # Rename columns\n  return(df)\n}, processed_data, names(processed_data))  # Use stored file names\n\n#merge files together by \"CpG\"\nmerged_data <- purrr::reduce(processed_data, full_join, by = \"CpG\")\n\n# Print structure of final merged data\nstr(merged_data)\n```\n:::\n\n\n\nThen changed things up. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#remove 225-T1\n\nmerged_data <- merged_data[, !(names(merged_data) %in% \"ACR-225-TP1_10x_processed.txt\")]\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Convert all columns (except \"CpG\") to numeric\nmerged_data <- merged_data %>%\n  mutate(across(-CpG, as.numeric)) \\\n```\n:::\n\n\n\nOnly keep CpGs that have a non-na value in all samples. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfiltered_wgbs <- na.omit(merged_data)\n\n# Ensure it's formatted as a data frame\nfiltered_wgbs <- as.data.frame(filtered_wgbs)\n# Only keep the sample information in the column name. \ncolnames(filtered_wgbs) <- gsub(\"^(.*?)_.*$\", \"\\\\1\", colnames(filtered_wgbs))\n# Set CpG IDs to rownames\nrownames(filtered_wgbs) <- filtered_wgbs$CpG\nfiltered_wgbs <- filtered_wgbs %>% select(-CpG)\n\nnrow(merged_data)\nnrow(filtered_wgbs)\n```\n:::\n\n\n\n\nWe had 12,093,025 CpGs before filtering and have 96785 after filtering.\n\n---\n\n\n# How about Ptua?\n\n\nFirst lets go back to processed txt loci file to see if there are any outliers.\n\nMissing data downloads - https://github.com/urol-e5/timeseries_molecular/issues/34\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}